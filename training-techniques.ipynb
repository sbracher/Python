{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Techniques\n",
    "Never use the same dataset to train your ML model and make predictions for evaluating the model (because this results in overfitting). Always evaluate the model on data that was not used to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these examples use the Pima Indian diabetes dataset\n",
    "url = \"pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate array into features (X) and label (y) parts\n",
    "X = array[:,0:8]\n",
    "y = array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Sets\n",
    "The simplest technique is to use different training and testing datasets by splitting our original dataset into 2 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.922%\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.3   # the testing part is 30% of the original dataset\n",
    "seed = 8\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "result = model.score(X_test, y_test)\n",
    "print(\"Accuracy: %.3f%%\" % (result*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation\n",
    "This technique gives less variance compared to the previous technique. It works by splitting the dataset into k-parts (each part is called a fold). The model is trained on k-1 folds, with one held back as the test set. This is repeated so that each fold has been used as the hold-back test set. At the end, we end up with k different performance scores that are then summarised using mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.951% (4.841%)\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 8\n",
    "\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Out Cross Validation\n",
    "This variation of cross validation works by setting the number of folds to the number of observations in the dataset (i.e. the number of obervations in each fold is one). It produces a large number of performance scores that can then be summarised to give a more reasonable estimate of the accuracy of the model. Downside is that this technique is computationally more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.823% (42.196%)\n"
     ]
    }
   ],
   "source": [
    "num_instances = len(X)\n",
    "\n",
    "loocv = cross_validation.LeaveOneOut(n=num_instances)\n",
    "model = LogisticRegression()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, y, cv=loocv)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique has more variance than k-fold cross validation (as shown above in the standard deviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Random Train-Test Splits\n",
    "Repeats the process of splitting the data and evaluating the model multiple times. Has the speed of the train/test split technique and the reduced variance in the k-fold cross validation technique. Downside is that repetitions may include much of the same data in the train or test split from run to run, introducing redundancy into the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.840% (1.909%)\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "test_size = 0.3\n",
    "num_instances = len(X)\n",
    "\n",
    "seed = 8\n",
    "kfold = cross_validation.ShuffleSplit(n=num_instances, n_iter=num_samples, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What technique to use?\n",
    "* K-fold cross validation is the \"best practice\" for evaluating the performance of a model on unseen data, with k set to 3, 5 or 10.\n",
    "* Train/test split is good for speed if using a slow algorithm and produces performance scores with lower bias when using large datasets.\n",
    "* If in doubt, use 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
